import pathlib

from gensim.models import Word2Vec as GensimWord2Vec

from .embedding_generator import AbstractEmbeddingGenerator
from ..config import Argument, IntArgument, EnumArgument, Config


class Word2VecGenerator(AbstractEmbeddingGenerator):
    def generate_embedding(self, issues: list[str], path: pathlib.Path, conf: Config):
        min_count = self.params["min-count"]
        vector_size = self.params["vector-length"]
        model = GensimWord2Vec(
            issues,
            min_count=min_count,
            vector_size=vector_size,
            sg=self.params["algorithm"] == "skip-gram",
            workers=conf.get("system.resources.threads"),
        )
        model.wv.save_word2vec_format(path, binary=True)

    @staticmethod
    def get_arguments() -> dict[str, Argument]:
        return super(Word2VecGenerator, Word2VecGenerator).get_arguments() | {
            "vector-length": IntArgument(
                name="vector-length",
                description="Size of the vectors generated by Doc2Vec",
                minimum=2,
                maximum=10000,
            ),
            "min-count": IntArgument(
                name="min-count",
                description="Minimum amount of occurrences for a word to be included",
                minimum=0,
                maximum=10000,
            ),
            "algorithm": EnumArgument(
                name="algorithm",
                description="Word2Vec algorithm to use",
                default="skip-gram",
                options=["cbow", "skip-gram"],
            ),
        }
